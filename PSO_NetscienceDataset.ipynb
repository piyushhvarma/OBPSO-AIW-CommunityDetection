{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRxkeWnfENy5",
        "outputId": "b19d9fe6-bc72-47f5-b438-8e21c6214ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading graph from local file (ca-netscience.mtx by skipping header lines)...\n",
            "Data Loaded Successfully: NetScience LCC has 379 authors and 914 edges.\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "import random\n",
        "import math\n",
        "import io\n",
        "\n",
        "# --- CONFIGURATION: ASSUMED LOCAL DATASET FILE ---\n",
        "FILENAME = \"ca-netscience.mtx\"\n",
        "# The code will now treat the content of this .mtx file as a simple edge list.\n",
        "\n",
        "# Define a list of expected header/comment characters to skip\n",
        "SKIP_CHARS = ('%', '#', '*')\n",
        "\n",
        "# --- 1. LOAD NETWORKX GRAPH (ROBUST EDGE LIST READING) ---\n",
        "print(f\"Loading graph from local file ({FILENAME} by skipping header lines)...\")\n",
        "if not os.path.exists(FILENAME):\n",
        "    raise FileNotFoundError(f\"Error: The file '{FILENAME}' was not found. Please check the file name and path.\")\n",
        "\n",
        "try:\n",
        "    # Filter file content to skip header/comment lines before loading\n",
        "    with open(FILENAME, 'r') as f:\n",
        "        # Filter out lines starting with comment characters AND ensure lines are not empty\n",
        "        edge_lines = [line for line in f if not line.strip().startswith(SKIP_CHARS) and line.strip()]\n",
        "\n",
        "    # Write the clean edge data to an in-memory file for NetworkX to read\n",
        "    clean_data = \"\\n\".join(edge_lines)\n",
        "\n",
        "    # Load the graph from the cleaned edge data. This is the fix for the .mtx formatting issue.\n",
        "    G_full = nx.read_edgelist(\n",
        "        io.StringIO(clean_data), # Use the in-memory string as a file\n",
        "        create_using=nx.Graph(),\n",
        "        nodetype=int,\n",
        "        data=False,\n",
        "        encoding='utf-8'\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    G_full = nx.Graph()\n",
        "    print(f\"FATAL ERROR: Failed to read file as an edge list. Check file content. Details: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. FILTER TO LARGEST CONNECTED COMPONENT (LCC) ---\n",
        "if G_full.number_of_nodes() == 0:\n",
        "    raise ValueError(\"The graph loaded zero nodes. Check the file content.\")\n",
        "\n",
        "largest_cc = max(nx.connected_components(G_full), key=len)\n",
        "G = G_full.subgraph(largest_cc).copy()\n",
        "\n",
        "# --- 3. CREATE INDEX MAPPING ---\n",
        "node_list = sorted(G.nodes())\n",
        "node_to_index = {node: i for i, node in enumerate(node_list)}\n",
        "index_to_node = {i: node for node, i in node_to_index.items()}\n",
        "NUM_AUTHORS = G.number_of_nodes()\n",
        "DIMENSION = NUM_AUTHORS\n",
        "\n",
        "print(f\"Data Loaded Successfully: NetScience LCC has {NUM_AUTHORS} authors and {G.number_of_edges()} edges.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_modularity(G, community_assignment_array):\n",
        "    \"\"\"\n",
        "    Calculates the Modularity (Q) of a community assignment.\n",
        "    \"\"\"\n",
        "    node_assignment = {index_to_node[i]: community_assignment_array[i]\n",
        "                       for i in range(NUM_AUTHORS)}\n",
        "\n",
        "    communities = {}\n",
        "    for node, c_id in node_assignment.items():\n",
        "        if c_id not in communities:\n",
        "            communities[c_id] = set()\n",
        "        communities[c_id].add(node)\n",
        "\n",
        "    partition = list(communities.values())\n",
        "\n",
        "    try:\n",
        "        Q = nx.community.modularity(G, partition)\n",
        "    except ZeroDivisionError:\n",
        "        Q = 0.0\n",
        "\n",
        "    return Q"
      ],
      "metadata": {
        "id": "yv-jhd8OF32L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PSO HYPERPARAMETERS ---\n",
        "NUM_PARTICLES = 30\n",
        "MAX_ITERATIONS = 50\n",
        "NUM_CLUSTERS = 10\n",
        "W_MAX = 0.9\n",
        "W_MIN = 0.4\n",
        "C1 = 2.0\n",
        "C2 = 2.0\n",
        "V_MAX = 6.0\n",
        "\n",
        "# --- PARTICLE CLASS ---\n",
        "class Particle:\n",
        "    def __init__(self, dimension, K):\n",
        "        self.dimension = dimension\n",
        "        self.K = K\n",
        "        self.position = np.random.randint(0, self.K, size=self.dimension)\n",
        "        self.velocity = np.zeros(self.dimension, dtype=float)\n",
        "        self.pbest_position = self.position.copy()\n",
        "        self.pbest_fitness = -1.0\n",
        "        self.current_fitness = -1.0\n",
        "\n",
        "# --- INITIALIZATION & MAIN DPSO LOOP ---\n",
        "print(f\"\\n--- Starting Discrete PSO for NetScience Community Detection (K={NUM_CLUSTERS}) ---\")\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "swarm = [Particle(DIMENSION, NUM_CLUSTERS) for _ in range(NUM_PARTICLES)]\n",
        "gbest_position = swarm[0].position.copy()\n",
        "gbest_fitness = -1.0\n",
        "\n",
        "for iter in range(MAX_ITERATIONS):\n",
        "    w = W_MAX - (W_MAX - W_MIN) * iter / MAX_ITERATIONS\n",
        "\n",
        "    for particle in swarm:\n",
        "        particle.current_fitness = calculate_modularity(G, particle.position)\n",
        "\n",
        "        if particle.current_fitness > particle.pbest_fitness:\n",
        "            particle.pbest_fitness = particle.current_fitness\n",
        "            particle.pbest_position = particle.position.copy()\n",
        "\n",
        "            if particle.pbest_fitness > gbest_fitness:\n",
        "                gbest_fitness = particle.pbest_fitness\n",
        "                gbest_position = particle.pbest_position.copy()\n",
        "\n",
        "    for particle in swarm:\n",
        "        r1 = np.random.rand(DIMENSION)\n",
        "        r2 = np.random.rand(DIMENSION)\n",
        "\n",
        "        cognitive_term = C1 * r1 * (particle.pbest_position - particle.position)\n",
        "        social_term = C2 * r2 * (gbest_position - particle.position)\n",
        "\n",
        "        new_velocity = w * particle.velocity + cognitive_term + social_term\n",
        "        new_velocity = np.clip(new_velocity, -V_MAX, V_MAX)\n",
        "        particle.velocity = new_velocity\n",
        "\n",
        "        for i in range(DIMENSION):\n",
        "            #\n",
        "            p_switch = 1.0 / (1.0 + math.exp(-particle.velocity[i]))\n",
        "\n",
        "            if random.random() < p_switch:\n",
        "                pbest_assignment = particle.pbest_position[i]\n",
        "                gbest_assignment = gbest_position[i]\n",
        "\n",
        "                if random.random() < 0.5:\n",
        "                    particle.position[i] = gbest_assignment\n",
        "                else:\n",
        "                    particle.position[i] = pbest_assignment\n",
        "\n",
        "            particle.position[i] = int(particle.position[i] % NUM_CLUSTERS)\n",
        "\n",
        "\n",
        "    print(f\"Iteration {iter+1}/{MAX_ITERATIONS}: Global Best Modularity = {gbest_fitness:.4f}\")\n",
        "\n",
        "# --- FINAL RESULT ---\n",
        "print(\"\\n--- OPTIMIZATION COMPLETE ---\")\n",
        "print(f\"Final Best Modularity Score: {gbest_fitness:.4f}\")\n",
        "\n",
        "final_assignments = {index_to_node[i]: gbest_position[i] for i in range(DIMENSION)}\n",
        "community_counts = Counter(final_assignments.values())\n",
        "print(f\"Number of distinct communities found: {len(community_counts)}\")\n",
        "print(\"Top 5 largest communities (size):\", community_counts.most_common(5))"
      ],
      "metadata": {
        "id": "oMAkV-kwF78b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37fa2223-08c8-4acb-e3e3-2e1e3c453363"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Discrete PSO for NetScience Community Detection (K=10) ---\n",
            "Iteration 1/50: Global Best Modularity = 0.0188\n",
            "Iteration 2/50: Global Best Modularity = 0.0188\n",
            "Iteration 3/50: Global Best Modularity = 0.0188\n",
            "Iteration 4/50: Global Best Modularity = 0.0200\n",
            "Iteration 5/50: Global Best Modularity = 0.0200\n",
            "Iteration 6/50: Global Best Modularity = 0.0276\n",
            "Iteration 7/50: Global Best Modularity = 0.0276\n",
            "Iteration 8/50: Global Best Modularity = 0.0286\n",
            "Iteration 9/50: Global Best Modularity = 0.0286\n",
            "Iteration 10/50: Global Best Modularity = 0.0293\n",
            "Iteration 11/50: Global Best Modularity = 0.0293\n",
            "Iteration 12/50: Global Best Modularity = 0.0342\n",
            "Iteration 13/50: Global Best Modularity = 0.0345\n",
            "Iteration 14/50: Global Best Modularity = 0.0345\n",
            "Iteration 15/50: Global Best Modularity = 0.0345\n",
            "Iteration 16/50: Global Best Modularity = 0.0438\n",
            "Iteration 17/50: Global Best Modularity = 0.0438\n",
            "Iteration 18/50: Global Best Modularity = 0.0438\n",
            "Iteration 19/50: Global Best Modularity = 0.0445\n",
            "Iteration 20/50: Global Best Modularity = 0.0494\n",
            "Iteration 21/50: Global Best Modularity = 0.0494\n",
            "Iteration 22/50: Global Best Modularity = 0.0522\n",
            "Iteration 23/50: Global Best Modularity = 0.0522\n",
            "Iteration 24/50: Global Best Modularity = 0.0522\n",
            "Iteration 25/50: Global Best Modularity = 0.0522\n",
            "Iteration 26/50: Global Best Modularity = 0.0522\n",
            "Iteration 27/50: Global Best Modularity = 0.0522\n",
            "Iteration 28/50: Global Best Modularity = 0.0522\n",
            "Iteration 29/50: Global Best Modularity = 0.0522\n",
            "Iteration 30/50: Global Best Modularity = 0.0522\n",
            "Iteration 31/50: Global Best Modularity = 0.0533\n",
            "Iteration 32/50: Global Best Modularity = 0.0621\n",
            "Iteration 33/50: Global Best Modularity = 0.0632\n",
            "Iteration 34/50: Global Best Modularity = 0.0658\n",
            "Iteration 35/50: Global Best Modularity = 0.0662\n",
            "Iteration 36/50: Global Best Modularity = 0.0662\n",
            "Iteration 37/50: Global Best Modularity = 0.0664\n",
            "Iteration 38/50: Global Best Modularity = 0.0695\n",
            "Iteration 39/50: Global Best Modularity = 0.0702\n",
            "Iteration 40/50: Global Best Modularity = 0.0777\n",
            "Iteration 41/50: Global Best Modularity = 0.0777\n",
            "Iteration 42/50: Global Best Modularity = 0.0784\n",
            "Iteration 43/50: Global Best Modularity = 0.0798\n",
            "Iteration 44/50: Global Best Modularity = 0.0798\n",
            "Iteration 45/50: Global Best Modularity = 0.0798\n",
            "Iteration 46/50: Global Best Modularity = 0.0812\n",
            "Iteration 47/50: Global Best Modularity = 0.0812\n",
            "Iteration 48/50: Global Best Modularity = 0.0815\n",
            "Iteration 49/50: Global Best Modularity = 0.0815\n",
            "Iteration 50/50: Global Best Modularity = 0.0818\n",
            "\n",
            "--- OPTIMIZATION COMPLETE ---\n",
            "Final Best Modularity Score: 0.0818\n",
            "Number of distinct communities found: 5\n",
            "Top 5 largest communities (size): [(np.int64(9), 223), (np.int64(8), 107), (np.int64(7), 39), (np.int64(6), 8), (np.int64(5), 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QkPjRUWKEJv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}