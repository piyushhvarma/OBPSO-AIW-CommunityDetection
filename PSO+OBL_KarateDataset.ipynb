{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRxkeWnfENy5",
        "outputId": "a307f0b2-c2bc-43dc-d9c3-5b95bda12399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Zachary's Karate Club Graph...\n",
            "Graph Loaded: Karate Club has 34 nodes.\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "import io\n",
        "\n",
        "# --- 1. LOAD NETWORKX GRAPH ---\n",
        "print(\"Loading Zachary's Karate Club Graph...\")\n",
        "G = nx.karate_club_graph()\n",
        "\n",
        "# --- 2. CREATE INDEX MAPPING ---\n",
        "# Ensure the node IDs are sequential from 0 to N-1 for array processing\n",
        "node_list = sorted(G.nodes())\n",
        "node_to_index = {node: i for i, node in enumerate(node_list)}\n",
        "index_to_node = {i: node for node, i in node_to_index.items()}\n",
        "NUM_NODES = G.number_of_nodes() # n in your theory\n",
        "DIMENSION = NUM_NODES\n",
        "\n",
        "# --- 3. HYPERPARAMETERS (Adjusted for a small graph) ---\n",
        "NUM_PARTICLES = 50     # P_n: Total population size\n",
        "MAX_ITERATIONS = 50    # N_max: Maximum iterations\n",
        "\n",
        "# PSO parameters (standard)\n",
        "W_MAX = 0.9\n",
        "W_MIN = 0.4\n",
        "C1 = 2.0\n",
        "C2 = 2.0\n",
        "V_MAX = 6.0\n",
        "\n",
        "# OBL/Crossover parameters\n",
        "A_i = 0                # Lower bound for node assignment (0-based indexing for cluster IDs)\n",
        "B_i = NUM_NODES - 1    # Upper bound for node assignment (Max possible cluster ID)\n",
        "NUM_CLUSTERS = 4       # Target number of communities (K) - Common for this network\n",
        "P_c = 0.2              # Ratio of crossover (20% of population)\n",
        "\n",
        "print(f\"Graph Loaded: Karate Club has {NUM_NODES} nodes.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_modularity(G, community_assignment_array):\n",
        "    \"\"\"Calculates the Modularity (Q) of a community assignment.\"\"\"\n",
        "\n",
        "    node_assignment = {}\n",
        "    for i in range(DIMENSION):\n",
        "        # Map array index 'i' (0 to N-1) to original node ID\n",
        "        node_id = index_to_node[i]\n",
        "\n",
        "        # Get the community ID assigned by the particle\n",
        "        community_id = community_assignment_array[i]\n",
        "\n",
        "        # Assign the community ID to the original node ID\n",
        "        node_assignment[node_id] = community_id\n",
        "\n",
        "    # Convert to NetworkX partition format: list of sets of nodes\n",
        "    communities = {}\n",
        "    for node, c_id in node_assignment.items():\n",
        "        if c_id not in communities:\n",
        "            communities[c_id] = set()\n",
        "        communities[c_id].add(node)\n",
        "\n",
        "    partition = list(communities.values())\n",
        "\n",
        "    if not partition or all(not c for c in partition):\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        Q = nx.community.modularity(G, partition)\n",
        "    except nx.exception.NotAPartition:\n",
        "        return -1.0\n",
        "    except ZeroDivisionError:\n",
        "        return 0.0\n",
        "\n",
        "    return Q"
      ],
      "metadata": {
        "id": "yv-jhd8OF32L"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MAIN HYBRID PSO LOOP ---\n",
        "for iter in range(MAX_ITERATIONS):\n",
        "    # Dynamic Inertia Weight\n",
        "    w = W_MAX - (W_MAX - W_MIN) * iter / MAX_ITERATIONS\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # PHASE 1: STANDARD PSO UPDATES (Movement)\n",
        "    # ----------------------------------------------------\n",
        "\n",
        "    # 1. Update Velocity and Position (DPSO Steps)\n",
        "    for particle in swarm:\n",
        "        # Standard PSO Movement... (same as before)\n",
        "        r1 = np.random.rand(DIMENSION)\n",
        "        r2 = np.random.rand(DIMENSION)\n",
        "\n",
        "        cognitive_term = C1 * r1 * (particle.pbest_position - particle.position)\n",
        "        social_term = C2 * r2 * (gbest_position - particle.position)\n",
        "\n",
        "        new_velocity = w * particle.velocity + cognitive_term + social_term\n",
        "        new_velocity = np.clip(new_velocity, -V_MAX, V_MAX)\n",
        "        particle.velocity = new_velocity\n",
        "\n",
        "        # Update Position (Discretization)\n",
        "        for i in range(DIMENSION):\n",
        "            p_switch = 1.0 / (1.0 + math.exp(-particle.velocity[i]))\n",
        "\n",
        "            if random.random() < p_switch:\n",
        "                pbest_assignment = particle.pbest_position[i]\n",
        "                gbest_assignment = gbest_position[i]\n",
        "\n",
        "                if random.random() < 0.5:\n",
        "                    particle.position[i] = gbest_assignment\n",
        "                else:\n",
        "                    particle.position[i] = pbest_assignment\n",
        "\n",
        "            particle.position[i] = int(particle.position[i] % NUM_CLUSTERS)\n",
        "\n",
        "        particle.current_fitness = calculate_modularity(G, particle.position)\n",
        "\n",
        "    # 2. Update Pbest and Gbest after movement\n",
        "    for particle in swarm:\n",
        "        if particle.current_fitness > particle.pbest_fitness:\n",
        "            particle.pbest_fitness = particle.current_fitness\n",
        "            particle.pbest_position = particle.position.copy()\n",
        "\n",
        "        if particle.pbest_fitness > gbest_fitness:\n",
        "            gbest_fitness = particle.pbest_fitness\n",
        "            gbest_position = particle.pbest_position.copy()\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # PHASE 2: CUSTOM CROSSOVER OPERATION (Elitism)\n",
        "    # ----------------------------------------------------\n",
        "\n",
        "    # Sort particles by current fitness in descending order to identify top parents\n",
        "    swarm.sort(key=lambda p: p.current_fitness, reverse=True)\n",
        "\n",
        "    num_crossover = int(NUM_PARTICLES * P_c)\n",
        "    if num_crossover % 2 != 0: num_crossover = max(2, num_crossover + 1) # Ensure even and at least 2\n",
        "\n",
        "    crossover_parents = swarm[:num_crossover]\n",
        "    new_offspring_swarm = []\n",
        "\n",
        "    # Paired Crossover: I1 & I_last, I2 & I_(last-1), etc.\n",
        "    for j in range(num_crossover // 2):\n",
        "        parent_A = crossover_parents[j]\n",
        "        parent_B = crossover_parents[num_crossover - 1 - j]\n",
        "\n",
        "        split_point = random.randint(1, DIMENSION - 1)\n",
        "\n",
        "        # Create two offspring positions\n",
        "        offspring_1_pos = np.concatenate((parent_A.position[:split_point], parent_B.position[split_point:]))\n",
        "        offspring_2_pos = np.concatenate((parent_B.position[:split_point], parent_A.position[split_point:]))\n",
        "\n",
        "        # Create new particles and evaluate their fitness\n",
        "        offspring_1 = Particle(DIMENSION, NUM_CLUSTERS, initial_position=offspring_1_pos)\n",
        "        offspring_1.current_fitness = calculate_modularity(G, offspring_1_pos)\n",
        "\n",
        "        offspring_2 = Particle(DIMENSION, NUM_CLUSTERS, initial_position=offspring_2_pos)\n",
        "        offspring_2.current_fitness = calculate_modularity(G, offspring_2_pos)\n",
        "\n",
        "        new_offspring_swarm.extend([offspring_1, offspring_2])\n",
        "\n",
        "    # 3. Elitist Selection (Combine Parents + Offspring and select the best N)\n",
        "    if new_offspring_swarm:\n",
        "        combined_population = swarm + new_offspring_swarm\n",
        "        combined_population.sort(key=lambda p: p.current_fitness, reverse=True)\n",
        "        # The new swarm is the best N_PARTICLES from the combined pool\n",
        "        swarm = combined_population[:NUM_PARTICLES]\n",
        "\n",
        "    # Final Gbest check from the newly selected swarm\n",
        "    current_gbest = max(swarm, key=lambda p: p.current_fitness)\n",
        "    if current_gbest.current_fitness > gbest_fitness:\n",
        "        gbest_fitness = current_gbest.current_fitness\n",
        "        gbest_position = current_gbest.position.copy()\n",
        "\n",
        "    print(f\"Iteration {iter+1}/{MAX_ITERATIONS}: Global Best Modularity = {gbest_fitness:.4f}\")\n",
        "\n",
        "# --- FINAL RESULT ---\n",
        "print(\"\\n--- OPTIMIZATION COMPLETE ---\")\n",
        "print(f\"Final Best Modularity Score: {gbest_fitness:.4f}\")\n",
        "\n",
        "final_assignments = {index_to_node[i]: gbest_position[i] for i in range(DIMENSION)}\n",
        "community_counts = Counter(final_assignments.values())\n",
        "print(f\"Number of distinct communities found: {len(community_counts)}\")\n",
        "print(\"Top 5 largest communities (size):\", community_counts.most_common(5))"
      ],
      "metadata": {
        "id": "oMAkV-kwF78b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "2129f171-9f63-4515-faea-a648b19163e7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (34,) (62,) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1512753792.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIMENSION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mcognitive_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbest_position\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mparticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0msocial_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgbest_position\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mparticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (34,) (62,) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ABz75rxBQ4QE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}