{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRxkeWnfENy5",
        "outputId": "69485677-afb1-41b0-f151-5b9f8567b25b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading College Football Network Graph...\n",
            "Graph Loaded: College Football Network has 115 teams.\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "import io\n",
        "import community.community_louvain as community_louvain # Needed for refinement\n",
        "\n",
        "# --- 1. LOAD NETWORKX GRAPH ---\n",
        "print(\"Loading College Football Network Graph...\")\n",
        "# The College Football network is a classic graph available in many repos.\n",
        "# We assume the file is named \"football.gml\"\n",
        "FILENAME_GML = \"football.gml\"\n",
        "try:\n",
        "    G = nx.read_gml(FILENAME_GML)\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(f\"Error: The file '{FILENAME_GML}' was not found. Please download the College Football network file.\")\n",
        "\n",
        "# --- 2. CREATE INDEX MAPPING ---\n",
        "# Ensure the node IDs are sequential from 0 to N-1 for array processing\n",
        "node_list = sorted(G.nodes())\n",
        "node_to_index = {node: i for i, node in enumerate(node_list)}\n",
        "index_to_node = {i: node for node, i in node_to_index.items()}\n",
        "NUM_TEAMS = G.number_of_nodes()\n",
        "DIMENSION = NUM_TEAMS\n",
        "\n",
        "# --- 3. HYPERPARAMETERS (Adjusted for conference detection) ---\n",
        "NUM_PARTICLES = 50     # P_n: Total population size\n",
        "MAX_ITERATIONS = 50    # N_max: Maximum iterations\n",
        "\n",
        "# PSO parameters (standard)\n",
        "W_MAX = 0.9\n",
        "W_MIN = 0.4\n",
        "C1 = 2.0\n",
        "C2 = 2.0\n",
        "V_MAX = 6.0\n",
        "\n",
        "# OBL/Crossover parameters\n",
        "A_i = 0\n",
        "B_i = NUM_TEAMS - 1\n",
        "NUM_CLUSTERS = 12      # Target number of communities (K) - Close to the actual number of conferences\n",
        "P_c = 0.2              # Ratio of crossover (20% of population)\n",
        "\n",
        "print(f\"Graph Loaded: College Football Network has {NUM_TEAMS} teams.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_modularity(G, community_assignment_array):\n",
        "    \"\"\"Calculates the Modularity (Q) of a community assignment.\"\"\"\n",
        "\n",
        "    node_assignment = {}\n",
        "    for i in range(DIMENSION):\n",
        "        node_id = index_to_node[i]\n",
        "        community_id = community_assignment_array[i]\n",
        "        node_assignment[node_id] = community_id\n",
        "\n",
        "    communities = {}\n",
        "    for node, c_id in node_assignment.items():\n",
        "        if c_id not in communities:\n",
        "            communities[c_id] = set()\n",
        "        communities[c_id].add(node)\n",
        "\n",
        "    partition = list(communities.values())\n",
        "\n",
        "    if not partition or all(not c for c in partition):\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        Q = nx.community.modularity(G, partition)\n",
        "    except nx.exception.NotAPartition:\n",
        "        # Penalty for invalid partition\n",
        "        return -1.0\n",
        "    except ZeroDivisionError:\n",
        "        return 0.0\n",
        "\n",
        "    return Q"
      ],
      "metadata": {
        "id": "yv-jhd8OF32L"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MAIN HYBRID PSO LOOP ---\n",
        "print(f\"\\n--- Starting Hybrid PSO for College Football Detection (K={NUM_CLUSTERS}) ---\")\n",
        "\n",
        "for iter in range(MAX_ITERATIONS):\n",
        "    w = W_MAX - (W_MAX - W_MIN) * iter / MAX_ITERATIONS\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # PHASE 1: STANDARD PSO UPDATES (Movement)\n",
        "    # ----------------------------------------------------\n",
        "    for particle in swarm:\n",
        "        # Update Pbest and Gbest before movement\n",
        "        if particle.current_fitness > particle.pbest_fitness:\n",
        "            particle.pbest_fitness = particle.current_fitness\n",
        "            particle.pbest_position = particle.position.copy()\n",
        "\n",
        "        if particle.pbest_fitness > gbest_fitness:\n",
        "            gbest_fitness = particle.pbest_fitness\n",
        "            gbest_position = particle.pbest_position.copy()\n",
        "\n",
        "        # PSO Velocity and Position Updates\n",
        "        r1 = np.random.rand(DIMENSION)\n",
        "        r2 = np.random.rand(DIMENSION)\n",
        "\n",
        "        cognitive_term = C1 * r1 * (particle.pbest_position - particle.position)\n",
        "        social_term = C2 * r2 * (gbest_position - particle.position)\n",
        "\n",
        "        new_velocity = w * particle.velocity + cognitive_term + social_term\n",
        "        new_velocity = np.clip(new_velocity, -V_MAX, V_MAX)\n",
        "        particle.velocity = new_velocity\n",
        "\n",
        "        # Discretization\n",
        "        for i in range(DIMENSION):\n",
        "            p_switch = 1.0 / (1.0 + math.exp(-particle.velocity[i]))\n",
        "\n",
        "            if random.random() < p_switch:\n",
        "                pbest_assignment = particle.pbest_position[i]\n",
        "                gbest_assignment = gbest_position[i]\n",
        "\n",
        "                if random.random() < 0.5:\n",
        "                    particle.position[i] = gbest_assignment\n",
        "                else:\n",
        "                    particle.position[i] = pbest_assignment\n",
        "\n",
        "            particle.position[i] = int(particle.position[i] % NUM_CLUSTERS)\n",
        "\n",
        "        particle.current_fitness = calculate_modularity(G, particle.position)\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # PHASE 2: CUSTOM CROSSOVER OPERATION (Elitist)\n",
        "    # ----------------------------------------------------\n",
        "\n",
        "    swarm.sort(key=lambda p: p.current_fitness, reverse=True)\n",
        "\n",
        "    num_crossover = int(NUM_PARTICLES * P_c)\n",
        "    if num_crossover % 2 != 0: num_crossover = max(2, num_crossover + 1)\n",
        "\n",
        "    crossover_parents = swarm[:num_crossover]\n",
        "    new_offspring_swarm = []\n",
        "\n",
        "    # Paired Crossover\n",
        "    for j in range(num_crossover // 2):\n",
        "        parent_A = crossover_parents[j]\n",
        "        parent_B = crossover_parents[num_crossover - 1 - j]\n",
        "\n",
        "        split_point = random.randint(1, DIMENSION - 1)\n",
        "\n",
        "        offspring_1_pos = np.concatenate((parent_A.position[:split_point], parent_B.position[split_point:]))\n",
        "        offspring_2_pos = np.concatenate((parent_B.position[:split_point], parent_A.position[split_point:]))\n",
        "\n",
        "        offspring_1 = Particle(DIMENSION, NUM_CLUSTERS, initial_position=offspring_1_pos)\n",
        "        offspring_1.current_fitness = calculate_modularity(G, offspring_1_pos)\n",
        "\n",
        "        offspring_2 = Particle(DIMENSION, NUM_CLUSTERS, initial_position=offspring_2_pos)\n",
        "        offspring_2.current_fitness = calculate_modularity(G, offspring_2_pos)\n",
        "\n",
        "        new_offspring_swarm.extend([offspring_1, offspring_2])\n",
        "\n",
        "    # 3. Elitist Selection (Combine Parents + Offspring and select the best N)\n",
        "    if new_offspring_swarm:\n",
        "        combined_population = swarm + new_offspring_swarm\n",
        "        combined_population.sort(key=lambda p: p.current_fitness, reverse=True)\n",
        "        swarm = combined_population[:NUM_PARTICLES]\n",
        "\n",
        "    # Final Gbest check from the newly selected swarm\n",
        "    current_gbest = max(swarm, key=lambda p: p.current_fitness)\n",
        "    if current_gbest.current_fitness > gbest_fitness:\n",
        "        gbest_fitness = current_gbest.current_fitness\n",
        "        gbest_position = current_gbest.position.copy()\n",
        "\n",
        "    print(f\"Iteration {iter+1}/{MAX_ITERATIONS}: Global Best Modularity = {gbest_fitness:.4f}\")\n",
        "\n",
        "# --- FINAL RESULT AND REFINEMENT ---\n",
        "\n",
        "# The best partition found by PSO (format: {node: community_id})\n",
        "pso_partition_dict = {index_to_node[i]: gbest_position[i] for i in range(DIMENSION)}\n",
        "\n",
        "print(\"\\n--- STARTING REFINEMENT (Louvain Local Search) ---\")\n",
        "\n",
        "# Run Louvain optimization starting from the PSO-found partition\n",
        "refined_partition_dict = community_louvain.best_partition(\n",
        "    G,\n",
        "    partition=pso_partition_dict,\n",
        "    resolution=1.0,\n",
        "    weight='weight'\n",
        ")\n",
        "\n",
        "# Convert the refined partition dictionary to the list-of-sets format for NetworkX modularity\n",
        "refined_communities_sets = {}\n",
        "for node, community_id in refined_partition_dict.items():\n",
        "    if community_id not in refined_communities_sets:\n",
        "        refined_communities_sets[community_id] = set()\n",
        "    refined_communities_sets[community_id].add(node)\n",
        "partition_for_modularity = list(refined_communities_sets.values())\n",
        "\n",
        "# 1. Calculate the final, refined Modularity Score\n",
        "refined_modularity = nx.community.modularity(G, partition_for_modularity)\n",
        "\n",
        "# 2. Analyze results\n",
        "gbest_modularity = gbest_fitness\n",
        "\n",
        "print(\"\\n--- FINAL OPTIMIZATION RESULTS ---\")\n",
        "print(f\"1. PSO Final Modularity: {gbest_modularity:.4f}\")\n",
        "print(f\"2. REFINED Final Modularity (Louvain Post-processing): {refined_modularity:.4f}\")\n",
        "\n",
        "if refined_modularity > gbest_modularity:\n",
        "    print(\"✨ Refinement IMPROVED the result!\")\n",
        "    final_modularity = refined_modularity\n",
        "    final_assignments = refined_partition_dict\n",
        "    final_community_counts = Counter(final_assignments.values())\n",
        "else:\n",
        "    print(\"⚠️ Refinement did NOT improve the result; using PSO's best.\")\n",
        "    final_modularity = gbest_modularity\n",
        "    final_assignments = pso_partition_dict\n",
        "    final_community_counts = Counter(final_assignments.values())\n",
        "\n",
        "# --- FINAL COMMUNITY MEMBERSHIP OUTPUT ---\n",
        "final_communities = {}\n",
        "for node, community_id in final_assignments.items():\n",
        "    if community_id not in final_communities:\n",
        "        final_communities[community_id] = []\n",
        "    final_communities[community_id].append(node)\n",
        "\n",
        "print(f\"\\nFinal Best Modularity Score: {final_modularity:.4f}\")\n",
        "print(f\"Total Communities: {len(final_communities)}\")\n",
        "print(\"Top 5 largest communities (size):\", final_community_counts.most_common(5))\n",
        "\n",
        "print(\"\\n--- COMMUNITY MEMBERSHIP EXAMPLES ---\")\n",
        "for cid, members in final_communities.items():\n",
        "    member_list = members[:10] + ['...'] if len(members) > 10 else members\n",
        "    print(f\"Community {cid} (Size: {len(members)}): {member_list}\")"
      ],
      "metadata": {
        "id": "oMAkV-kwF78b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edc4f69-7903-4151-e953-164e5e8edb69"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Hybrid PSO for College Football Detection (K=12) ---\n",
            "Iteration 1/50: Global Best Modularity = 0.0286\n",
            "Iteration 2/50: Global Best Modularity = 0.0330\n",
            "Iteration 3/50: Global Best Modularity = 0.0381\n",
            "Iteration 4/50: Global Best Modularity = 0.0526\n",
            "Iteration 5/50: Global Best Modularity = 0.0634\n",
            "Iteration 6/50: Global Best Modularity = 0.0746\n",
            "Iteration 7/50: Global Best Modularity = 0.0886\n",
            "Iteration 8/50: Global Best Modularity = 0.0909\n",
            "Iteration 9/50: Global Best Modularity = 0.1000\n",
            "Iteration 10/50: Global Best Modularity = 0.1010\n",
            "Iteration 11/50: Global Best Modularity = 0.1036\n",
            "Iteration 12/50: Global Best Modularity = 0.1060\n",
            "Iteration 13/50: Global Best Modularity = 0.1095\n",
            "Iteration 14/50: Global Best Modularity = 0.1095\n",
            "Iteration 15/50: Global Best Modularity = 0.1109\n",
            "Iteration 16/50: Global Best Modularity = 0.1109\n",
            "Iteration 17/50: Global Best Modularity = 0.1109\n",
            "Iteration 18/50: Global Best Modularity = 0.1109\n",
            "Iteration 19/50: Global Best Modularity = 0.1109\n",
            "Iteration 20/50: Global Best Modularity = 0.1109\n",
            "Iteration 21/50: Global Best Modularity = 0.1109\n",
            "Iteration 22/50: Global Best Modularity = 0.1109\n",
            "Iteration 23/50: Global Best Modularity = 0.1109\n",
            "Iteration 24/50: Global Best Modularity = 0.1109\n",
            "Iteration 25/50: Global Best Modularity = 0.1109\n",
            "Iteration 26/50: Global Best Modularity = 0.1109\n",
            "Iteration 27/50: Global Best Modularity = 0.1109\n",
            "Iteration 28/50: Global Best Modularity = 0.1109\n",
            "Iteration 29/50: Global Best Modularity = 0.1109\n",
            "Iteration 30/50: Global Best Modularity = 0.1109\n",
            "Iteration 31/50: Global Best Modularity = 0.1109\n",
            "Iteration 32/50: Global Best Modularity = 0.1109\n",
            "Iteration 33/50: Global Best Modularity = 0.1109\n",
            "Iteration 34/50: Global Best Modularity = 0.1109\n",
            "Iteration 35/50: Global Best Modularity = 0.1109\n",
            "Iteration 36/50: Global Best Modularity = 0.1109\n",
            "Iteration 37/50: Global Best Modularity = 0.1109\n",
            "Iteration 38/50: Global Best Modularity = 0.1109\n",
            "Iteration 39/50: Global Best Modularity = 0.1109\n",
            "Iteration 40/50: Global Best Modularity = 0.1109\n",
            "Iteration 41/50: Global Best Modularity = 0.1109\n",
            "Iteration 42/50: Global Best Modularity = 0.1109\n",
            "Iteration 43/50: Global Best Modularity = 0.1109\n",
            "Iteration 44/50: Global Best Modularity = 0.1109\n",
            "Iteration 45/50: Global Best Modularity = 0.1109\n",
            "Iteration 46/50: Global Best Modularity = 0.1109\n",
            "Iteration 47/50: Global Best Modularity = 0.1109\n",
            "Iteration 48/50: Global Best Modularity = 0.1109\n",
            "Iteration 49/50: Global Best Modularity = 0.1109\n",
            "Iteration 50/50: Global Best Modularity = 0.1109\n",
            "\n",
            "--- STARTING REFINEMENT (Louvain Local Search) ---\n",
            "\n",
            "--- FINAL OPTIMIZATION RESULTS ---\n",
            "1. PSO Final Modularity: 0.1109\n",
            "2. REFINED Final Modularity (Louvain Post-processing): 0.5131\n",
            "✨ Refinement IMPROVED the result!\n",
            "\n",
            "Final Best Modularity Score: 0.5131\n",
            "Total Communities: 6\n",
            "Top 5 largest communities (size): [(3, 31), (5, 26), (1, 23), (4, 16), (2, 10)]\n",
            "\n",
            "--- COMMUNITY MEMBERSHIP EXAMPLES ---\n",
            "Community 5 (Size: 26): ['BrighamYoung', 'KansasState', 'NewMexico', 'TexasTech', 'SanDiegoState', 'Baylor', 'NorthernIllinois', 'WesternMichigan', 'Wyoming', 'Utah', '...']\n",
            "Community 4 (Size: 16): ['FloridaState', 'Akron', 'NorthCarolinaState', 'BowlingGreenState', 'Virginia', 'Buffalo', 'GeorgiaTech', 'Duke', 'Kent', 'MiamiOhio', '...']\n",
            "Community 1 (Size: 23): ['Iowa', 'PennState', 'Northwestern', 'Wisconsin', 'Auburn', 'Alabama', 'Florida', 'Michigan', 'Purdue', 'OhioState', '...']\n",
            "Community 2 (Size: 10): ['SouthernCalifornia', 'ArizonaState', 'UCLA', 'Arizona', 'Washington', 'Oregon', 'Stanford', 'WashingtonState', 'OregonState', 'California']\n",
            "Community 3 (Size: 31): ['NorthTexas', 'VirginiaTech', 'ArkansasState', 'BoiseState', 'BostonCollege', 'WestVirginia', 'Syracuse', 'CentralFlorida', 'Connecticut', 'EastCarolina', '...']\n",
            "Community 0 (Size: 9): ['FresnoState', 'Rice', 'SouthernMethodist', 'Nevada', 'SanJoseState', 'TexasElPaso', 'Tulsa', 'TexasChristian', 'Hawaii']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ABz75rxBQ4QE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}